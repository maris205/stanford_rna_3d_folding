{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ed1f31-3f2b-4adc-8d77-3c4c7b2a500f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 设置环境变量\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "# 打印环境变量以确认设置成功\n",
    "print(os.environ.get('HF_ENDPOINT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c268962e-04fc-4e15-ad2f-373f5d905ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 20:39:08.963201: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-09 20:39:08.975167: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-09 20:39:08.989704: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-09 20:39:08.994092: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-09 20:39:09.004966: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-09 20:39:09.956676: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import GPT2LMHeadModel, AutoConfig,GPT2Tokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61973dd8-eac4-440d-b3fd-3dcb79e1e42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"dnagpt/gene_eng_gpt2_v0\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "969af9ae-299b-4846-bddd-5cebc7d9dae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at dnagpt/gene_eng_gpt2_v0 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#set model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"dnagpt/gene_eng_gpt2_v0\", num_labels=3, problem_type=\"regression\")\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e84a4b6-d271-4870-90d6-b105e2b597f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['seq', 'label'],\n",
       "        num_rows: 123385\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['seq', 'label'],\n",
       "        num_rows: 13710\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset,DatasetDict\n",
    "# 1. load ~11k samples from promoters prediction dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"rna_pos_1024_train.jsonl\")['train'].train_test_split(test_size=0.1)\n",
    "# dataset_val = load_dataset(\"json\", data_files=\"rna_pos_1024_val.jsonl\")\n",
    "\n",
    "# dataset = DatasetDict({\n",
    "#     \"train\":dataset_train[\"train\"],\n",
    "#     \"test\":dataset_val[\"train\"] }\n",
    "# )\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f351e3c-0e8c-4085-be94-bb03771cc1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seq': 'GGACGAGGAAACACTTGGGCGGCTTCGAAAGATGCCTATTTCGACCGGTGTTTGCGGGTCAAGAAGCGACATGGACCCCAAAGATCGGCTTGGTCTGACGAAGTGAATGGCGAGGTAATAAGTAGAGTCACCAAGACCCTCTTATCCACAGCTAGTGCTATTTTTGTATTTAGGTTAGCTATTTAGCTTTACGTTCCAGGATGCCTAGTGGCAGCCCCACAATATCCAGG',\n",
       " 'label': [-54.77099990844727, -43.78300094604492, 19.11100006103516]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][1242]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38a7bcaf-7aed-4e75-bde1-2d4aa9580409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets  mean token lenght 106.6472647702407 min token length 1 max token length 199\n"
     ]
    }
   ],
   "source": [
    "token_len_list = []\n",
    "for item in dataset[\"test\"]:\n",
    "    inputs = tokenizer.tokenize(item[\"seq\"])\n",
    "    token_len_list.append( len(inputs) )\n",
    "\n",
    "mean_len = sum(token_len_list)/len(token_len_list)\n",
    "min_len  = min(token_len_list)\n",
    "max_len = max(token_len_list)\n",
    "\n",
    "print(\"datasets \", \"mean token lenght\", mean_len, \"min token length\", min_len, \"max token length\", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e137c2c-943e-41df-b940-bacecb52400b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e27012a34f4081921036a50c0be096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/123385 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8449c9734f415bb6cbf1f1bca56598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/13710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. tokenize\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['seq'], truncation=True, padding='max_length',max_length=256)\n",
    "\n",
    "# 3. 对数据集应用分词函数\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=8)\n",
    "\n",
    "# 4. 创建一个数据收集器，用于动态填充和遮蔽\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69bb228e-374e-4404-88a5-8cd3c1a005a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    rmse = mean_squared_error(labels, predictions)\n",
    "    return {\"rmse\": rmse}\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",            # 每个 epoch 保存一次模型\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=20,\n",
    "    per_device_eval_batch_size=20,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,               # 最多保存 3 个 checkpoint\n",
    "    load_best_model_at_end=True,      # 训练结束时加载最好的模型\n",
    "    metric_for_best_model=\"eval_loss\", # 使用 eval_loss 作为评估指标\n",
    "    greater_is_better=False,          # eval_loss 越小越好\n",
    ")\n",
    "\n",
    "# 使用Trainer API进行训练（假设已有train_dataset和eval_dataset）\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "037fe6cf-b386-4683-9a37-1f1d087c8a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='61700' max='61700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [61700/61700 2:33:33, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3318.435000</td>\n",
       "      <td>3583.534668</td>\n",
       "      <td>3583.526367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2887.723800</td>\n",
       "      <td>3022.019287</td>\n",
       "      <td>3022.016846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2416.679500</td>\n",
       "      <td>2614.540039</td>\n",
       "      <td>2614.532227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2109.736000</td>\n",
       "      <td>2299.857666</td>\n",
       "      <td>2299.853271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1950.791600</td>\n",
       "      <td>2125.344971</td>\n",
       "      <td>2125.338623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1737.462700</td>\n",
       "      <td>2055.251953</td>\n",
       "      <td>2055.248291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1661.509900</td>\n",
       "      <td>2073.966064</td>\n",
       "      <td>2073.960938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1592.228700</td>\n",
       "      <td>2041.346558</td>\n",
       "      <td>2041.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1566.503500</td>\n",
       "      <td>2073.028809</td>\n",
       "      <td>2073.024658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1549.163000</td>\n",
       "      <td>2089.218018</td>\n",
       "      <td>2089.208740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=61700, training_loss=2194.6143567666127, metrics={'train_runtime': 9213.6248, 'train_samples_per_second': 133.916, 'train_steps_per_second': 6.697, 'total_flos': 1.612019533676544e+17, 'train_loss': 2194.6143567666127, 'epoch': 10.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19b728bc-0175-482a-b67f-683593c5b1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-61.452217, -55.75406 , -26.49135 ],\n",
       "       [188.73888 , 191.95325 , 224.40453 ],\n",
       "       [105.570244, 102.61758 , 124.64346 ],\n",
       "       ...,\n",
       "       [188.68427 , 205.40616 , 187.1908  ],\n",
       "       [201.12808 , 226.905   , 252.09097 ],\n",
       "       [-20.086788, -12.286413,   2.185144]], dtype=float32), label_ids=array([[-55.652, -52.663, -34.913],\n",
       "       [182.877, 206.286, 238.595],\n",
       "       [101.682, 111.573, 118.957],\n",
       "       ...,\n",
       "       [185.887, 205.243, 189.923],\n",
       "       [235.199, 225.855, 299.421],\n",
       "       [-26.234,  -1.115,   3.715]], dtype=float32), metrics={'test_loss': 2041.3465576171875, 'test_rmse': 2041.34375, 'test_runtime': 32.623, 'test_samples_per_second': 420.256, 'test_steps_per_second': 21.028})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#模型测试\n",
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c5518da-05fe-4f3e-8dc1-566947ec5bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./gene_eng_gpt2_v0_rna3d_ft_v1/tokenizer_config.json',\n",
       " './gene_eng_gpt2_v0_rna3d_ft_v1/special_tokens_map.json',\n",
       " './gene_eng_gpt2_v0_rna3d_ft_v1/vocab.json',\n",
       " './gene_eng_gpt2_v0_rna3d_ft_v1/merges.txt',\n",
       " './gene_eng_gpt2_v0_rna3d_ft_v1/added_tokens.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存模型和 tokenizer 到本地目录\n",
    "model.save_pretrained(\"./gene_eng_gpt2_v0_rna3d_ft_v1\")  # 模型保存到 ./my_model 目录\n",
    "tokenizer.save_pretrained(\"./gene_eng_gpt2_v0_rna3d_ft_v1\")  # tokenizer 保存到 ./my_model 目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7838bc79-ef31-4027-b1f4-8da017c37f02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
